{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP8o/bDn7nPr7WOmWUs2jTw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hanzlah10/CFA/blob/main/CFA_Major_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IGvf-I0wSinM"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import shap\n",
        "from sklearn.ensemble import RandomForestClassifier, IsolationForest\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import shap\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "def analyze_data_distribution(X, y, feature_names):\n",
        "    \"\"\"Show class distribution and feature importance\"\"\"\n",
        "    # Class distribution\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    unique, counts = np.unique(y, return_counts=True)\n",
        "    plt.bar(['Class ' + str(i) for i in unique], counts)\n",
        "    plt.title('Class Distribution')\n",
        "    plt.show()\n",
        "\n",
        "    # Feature distributions\n",
        "    fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
        "    for i, (ax, name) in enumerate(zip(axes.ravel(), feature_names)):\n",
        "        for label in [0, 1]:\n",
        "            ax.hist(X[y == label, i], alpha=0.5, label=f'Class {label}')\n",
        "        ax.set_title(name)\n",
        "        ax.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def get_important_features(X, y, feature_names):\n",
        "    \"\"\"Get feature importance using Random Forest\"\"\"\n",
        "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    rf.fit(X, y)\n",
        "\n",
        "    # Plot feature importance\n",
        "    importance = rf.feature_importances_\n",
        "    indices = np.argsort(importance)[::-1]\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.title('Feature Importance')\n",
        "    plt.bar(range(X.shape[1]), importance[indices])\n",
        "    plt.xticks(range(X.shape[1]), [feature_names[i] for i in indices], rotation=45)\n",
        "    plt.show()\n",
        "\n",
        "    # Print importance scores\n",
        "    print(\"\\nFeature Importance Scores:\")\n",
        "    for idx in indices:\n",
        "        print(f\"{feature_names[idx]}: {importance[idx]:.4f}\")\n",
        "\n",
        "    return indices[importance[indices] > 0.05]\n",
        "\n",
        "def visualize_results_with_connections(X_original, X_synthetic, y_original, feature_names, used_indices):\n",
        "    \"\"\"Visualize original and synthetic data using PCA and show connections\"\"\"\n",
        "    # Combine all data\n",
        "    X_combined = np.vstack([X_original, X_synthetic])\n",
        "\n",
        "    # Apply PCA\n",
        "    scaler = StandardScaler()\n",
        "    pca = PCA(n_components=2)\n",
        "    X_pca = pca.fit_transform(scaler.fit_transform(X_combined))\n",
        "\n",
        "    # Split back into original and synthetic\n",
        "    X_pca_orig = X_pca[:len(X_original)]\n",
        "    X_pca_synt = X_pca[len(X_original):]\n",
        "\n",
        "    # Print feature contributions for the first two principal components\n",
        "    for i, component in enumerate(pca.components_[:2], start=1):\n",
        "        sorted_indices = np.argsort(np.abs(component))[::-1]\n",
        "        sorted_features = [(feature_names[idx], component[idx]) for idx in sorted_indices]\n",
        "        print(f\"Principal Component {i}:\")\n",
        "        for feature, contribution in sorted_features:\n",
        "            print(f\"  {feature}: {contribution:.4f}\")\n",
        "\n",
        "    # Plot original and synthetic points\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.scatter(X_pca_orig[y_original == 0, 0], X_pca_orig[y_original == 0, 1],\n",
        "                label='Original Majority', alpha=0.6)\n",
        "    plt.scatter(X_pca_orig[y_original == 1, 0], X_pca_orig[y_original == 1, 1],\n",
        "                label='Original Minority', alpha=0.6)\n",
        "    plt.scatter(X_pca_synt[:, 0], X_pca_synt[:, 1],\n",
        "                label='Synthetic', marker='*', s=100, alpha=0.6)\n",
        "\n",
        "    # Draw lines connecting original majority points to their synthetic counterparts\n",
        "    majority_indices = np.where(y_original == 0)[0]\n",
        "    for syn_idx, maj_idx in enumerate(used_indices):\n",
        "        plt.plot([X_pca_orig[majority_indices[maj_idx], 0], X_pca_synt[syn_idx, 0]],\n",
        "                 [X_pca_orig[majority_indices[maj_idx], 1], X_pca_synt[syn_idx, 1]], 'k--', alpha=0.6)\n",
        "\n",
        "    plt.legend()\n",
        "    plt.title('PCA Visualization of Original and Synthetic Data with Connections')\n",
        "    plt.xlabel('Principal Component 1')\n",
        "    plt.ylabel('Principal Component 2')\n",
        "    plt.show()\n",
        "\n",
        "def generate_counterfactuals(X, y, feature_names, fd=2, tol=0.2):\n",
        "    \"\"\"Main CFA function\"\"\"\n",
        "    print(\"1. Analyzing Original Data Distribution\")\n",
        "    analyze_data_distribution(X, y, feature_names)\n",
        "\n",
        "    print(\"\\n2. Selecting Important Features\")\n",
        "    important_features = get_important_features(X, y, feature_names)\n",
        "    X_important = X[:, important_features]\n",
        "\n",
        "    # Calculate tolerance\n",
        "    tolerance = tol * np.std(X_important, axis=0)\n",
        "    print(\"\\nTolerance levels for important features:\")\n",
        "    for i, feat_idx in enumerate(important_features):\n",
        "        print(f\"{feature_names[feat_idx]}: {tolerance[i]:.4f}\")\n",
        "\n",
        "    # Separate classes\n",
        "    majority_mask = y == 0\n",
        "    majority_data = X_important[majority_mask]\n",
        "    minority_data = X_important[~majority_mask]\n",
        "\n",
        "    majority_indices = np.where(majority_mask)[0]\n",
        "    print(f\"\\nClass distribution:\")\n",
        "    print(f\"Majority class: {len(majority_data)}\")\n",
        "    print(f\"Minority class: {len(minority_data)}\")\n",
        "\n",
        "    # Generate synthetic instances\n",
        "    synthetic_instances = []\n",
        "    used_indices = []\n",
        "    majority_indices_used = []\n",
        "\n",
        "    # Find pairs using nearest neighbors\n",
        "    nn = NearestNeighbors(n_neighbors=1)\n",
        "    nn.fit(majority_data)\n",
        "\n",
        "    for min_instance in minority_data:\n",
        "        dist, maj_idx = nn.kneighbors([min_instance])\n",
        "        maj_idx = maj_idx[0][0]\n",
        "\n",
        "        if maj_idx not in used_indices:\n",
        "            diff = np.abs(min_instance - majority_data[maj_idx])\n",
        "            if np.sum(diff <= tolerance) >= fd:\n",
        "                synthetic = majority_data[maj_idx] + (min_instance - majority_data[maj_idx])\n",
        "                synthetic_instances.append(synthetic)\n",
        "                used_indices.append(maj_idx)\n",
        "                majority_indices_used.append(majority_indices[maj_idx])\n",
        "\n",
        "    if not synthetic_instances:\n",
        "        print(\"\\nNo synthetic instances generated. Try adjusting parameters.\")\n",
        "        return X, y, None\n",
        "\n",
        "    synthetic_instances = np.array(synthetic_instances)\n",
        "\n",
        "    # Create full synthetic instances\n",
        "    full_synthetic = np.zeros((len(synthetic_instances), X.shape[1]))\n",
        "    for i, maj_idx in enumerate(used_indices):\n",
        "        full_synthetic[i] = X[majority_indices[maj_idx]]\n",
        "        full_synthetic[i, important_features] = synthetic_instances[i]\n",
        "\n",
        "    print(f\"\\nGenerated {len(synthetic_instances)} synthetic instances\")\n",
        "\n",
        "    # Visualize results\n",
        "    print(\"\\n3. Visualizing Results\")\n",
        "    visualize_results_with_connections(X, full_synthetic, y, feature_names, used_indices)\n",
        "\n",
        "    # Create a table with 20 examples of synthetic data\n",
        "    synthetic_data_table = pd.DataFrame(full_synthetic[:20], columns=feature_names)\n",
        "    majority_data_table = pd.DataFrame(X[majority_indices_used[:20]], columns=feature_names)\n",
        "\n",
        "    differences = synthetic_data_table - majority_data_table\n",
        "    differences.columns = [col + '_change' for col in differences.columns]\n",
        "\n",
        "    table_with_changes = pd.concat([synthetic_data_table, differences], axis=1)\n",
        "\n",
        "    print(\"\\nTable with 20 examples of synthetic data and changes made:\")\n",
        "    print(table_with_changes)\n",
        "\n",
        "    # Return augmented dataset\n",
        "    X_augmented = np.vstack([X, full_synthetic])\n",
        "    y_augmented = np.concatenate([y, np.ones(len(full_synthetic))])\n",
        "    synthetic_labels = np.concatenate([np.zeros(len(X)), np.ones(len(full_synthetic))])\n",
        "\n",
        "    return X_augmented, y_augmented, synthetic_labels\n",
        "\n",
        "\n",
        "def remove_outliers(X, y):\n",
        "    \"\"\"Remove outliers using Isolation Forest\"\"\"\n",
        "    # Fit the Isolation Forest model\n",
        "    iso_forest = IsolationForest(contamination=0.1, random_state=42)\n",
        "    yhat = iso_forest.fit_predict(X)\n",
        "\n",
        "    # Select all rows that are not outliers\n",
        "    mask = yhat != -1\n",
        "    return X[mask], y[mask]\n",
        "\n",
        "def run_example_with_shap_updated():\n",
        "    # Load data\n",
        "    url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv'\n",
        "    columns = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome']\n",
        "\n",
        "    data = pd.read_csv(url, names=columns)\n",
        "    # data = data.sample(n=200, random_state=42)\n",
        "\n",
        "    # Separate features and target\n",
        "    X = data.drop('Outcome', axis=1).values\n",
        "    y = data['Outcome'].values\n",
        "    feature_names = list(data.drop('Outcome', axis=1).columns)\n",
        "\n",
        "    # Remove outliers\n",
        "    X, y = remove_outliers(X, y)\n",
        "\n",
        "    # Fit Random Forest model\n",
        "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    rf.fit(X, y)\n",
        "\n",
        "    # Explain using SHAP\n",
        "    print(\"Explaining using SHAP:\")\n",
        "    explain_with_shap(X, rf, feature_names)\n",
        "\n",
        "    # Run CFA (Counterfactual Augmentation)\n",
        "    X_augmented, y_augmented, synthetic_labels = generate_counterfactuals(\n",
        "        X, y, feature_names, fd=2, tol=0.2\n",
        "    )\n",
        "\n",
        "    print(\"\\nFinal Results:\")\n",
        "    print(\"-\" * 69)\n",
        "    print(f\"Original dataset shape (after outlier removal): {X.shape}\")\n",
        "    print(f\"Augmented dataset shape: {X_augmented.shape}\")\n",
        "    print(f\"Number of synthetic instances: {np.sum(synthetic_labels)}\")\n",
        "\n",
        "# Now you can run the updated example with the outlier removal included\n",
        "run_example_with_shap_updated()\n"
      ]
    }
  ]
}